{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"resop : re mote s hell op erational API # resop is a RESTful API to manage user, admin, job, (any) shell operations in a remote Linux cluster. It is specifically targeted to a supercomputer / HPC cluster but it can be used for any machine accessible via SSH. It is: A thin layer over classic HPC (and not) Linux clusters to make them easily accessible to third party applications, or anything talking HTTP. An SSH client with a library of common HPC commands and operations A tool to turn your long, manual command routines into a single POST request. High-level interaction diagram of the resop API and some typical applications Features # Run remote Linux shell commands through HTTP directly to a remote cluster and get a JSON response. Commands and arbitrary JavaScript are chained together to create more complex Operations that can be run in the same way as commands. A library of operations of typical HPC commands and routines is organized into operators. Some examples: slurm, ldap, utils Simple customisation : you can implement custom operations without dealing with API logic. Just create a JavaScript file using the provided class, list series of commands or use JavaScript power for server-side operations such as parsing strings. Secure commands and operations : every endpoint is accessed prior token-based authentication and will run remote operations with the privildeges of the corresponding authenticated user. Quick setup : NodeJS and MariaDB* are the only dependencies, one file configuration and the API is ready to be used Run anywhere : resop is stateless and uses remote SSH authentication so it can be hosted anywhere that has access to the HPC cluster Connect 3rd party apps : as any other RESTful API, you can easily connect to web or destop apps (portals, IDEs, local CLI apps etc.). Call the API from any programming language -> read the result -> done! *only needed if the logging feature is enabled Use-cases # Administration routines # I am a admin of an HPC center and I want to automate some recurrent operations that are done manually. In order to create or delete a user, I need to update different services: the system user database (LDAP), the scheduler database (Slurm) and the database for the online portal for remote visualization (PostgresSQL). The procedure to create an account becomes: $ ssh admin@hpccenter # ssh into the system $ vim new_user.ldif # write user details to LDIF $ ldapadd -f new_user.ldif # add the new user the LDAP database $ sacctmgr create user ... # add the user to slurmdbd # go to the web portal and add the new user With resop, I can group all this steps into a custom operation in my custom operator that I call: hpccenter->createUser. I will then just call: curl -X POST -d \"username=value1&password=value2\" https://resop:3000/hpcuser/opn/createUser Or I can call the API endpoint from within a basic HTML form page, or put it in a script on my local machine Developer # I am developing a portal to facilitate user access to \"HPCcluster\" and I want to display a monitoring graphical interface showing available nodes with hardware information displayed when clicking on a node. I can fetch all the infomation by calling resop from the portal backend, for instance, by calling the batch scheduler (such as Slurm) informative command. I will then receive a JSON object as a response that I can parse and use for my graphical interface. This approach is language-neutral so it is compatible with any programming language my application is written in. If the functionality that I need it is not in resop library, I can also implement my personal operation in resop, in a single JavaScript file. HPC user / researcher # I am an HPC user that needs to run a batch of simulations with different initial conditions. I can use resop to submit multiple jobs from my machine using postman or other tools that facilitate the interaction with an API, without having to connect to the cluster directly. (in future versions) I use the loggin functionality of the API to keep track of my past simulations and organize them.","title":"Home"},{"location":"#resop-remote-shell-operational-api","text":"resop is a RESTful API to manage user, admin, job, (any) shell operations in a remote Linux cluster. It is specifically targeted to a supercomputer / HPC cluster but it can be used for any machine accessible via SSH. It is: A thin layer over classic HPC (and not) Linux clusters to make them easily accessible to third party applications, or anything talking HTTP. An SSH client with a library of common HPC commands and operations A tool to turn your long, manual command routines into a single POST request. High-level interaction diagram of the resop API and some typical applications","title":"resop: remote shell operational API"},{"location":"#features","text":"Run remote Linux shell commands through HTTP directly to a remote cluster and get a JSON response. Commands and arbitrary JavaScript are chained together to create more complex Operations that can be run in the same way as commands. A library of operations of typical HPC commands and routines is organized into operators. Some examples: slurm, ldap, utils Simple customisation : you can implement custom operations without dealing with API logic. Just create a JavaScript file using the provided class, list series of commands or use JavaScript power for server-side operations such as parsing strings. Secure commands and operations : every endpoint is accessed prior token-based authentication and will run remote operations with the privildeges of the corresponding authenticated user. Quick setup : NodeJS and MariaDB* are the only dependencies, one file configuration and the API is ready to be used Run anywhere : resop is stateless and uses remote SSH authentication so it can be hosted anywhere that has access to the HPC cluster Connect 3rd party apps : as any other RESTful API, you can easily connect to web or destop apps (portals, IDEs, local CLI apps etc.). Call the API from any programming language -> read the result -> done! *only needed if the logging feature is enabled","title":"Features"},{"location":"#use-cases","text":"","title":"Use-cases"},{"location":"#administration-routines","text":"I am a admin of an HPC center and I want to automate some recurrent operations that are done manually. In order to create or delete a user, I need to update different services: the system user database (LDAP), the scheduler database (Slurm) and the database for the online portal for remote visualization (PostgresSQL). The procedure to create an account becomes: $ ssh admin@hpccenter # ssh into the system $ vim new_user.ldif # write user details to LDIF $ ldapadd -f new_user.ldif # add the new user the LDAP database $ sacctmgr create user ... # add the user to slurmdbd # go to the web portal and add the new user With resop, I can group all this steps into a custom operation in my custom operator that I call: hpccenter->createUser. I will then just call: curl -X POST -d \"username=value1&password=value2\" https://resop:3000/hpcuser/opn/createUser Or I can call the API endpoint from within a basic HTML form page, or put it in a script on my local machine","title":"Administration routines"},{"location":"#developer","text":"I am developing a portal to facilitate user access to \"HPCcluster\" and I want to display a monitoring graphical interface showing available nodes with hardware information displayed when clicking on a node. I can fetch all the infomation by calling resop from the portal backend, for instance, by calling the batch scheduler (such as Slurm) informative command. I will then receive a JSON object as a response that I can parse and use for my graphical interface. This approach is language-neutral so it is compatible with any programming language my application is written in. If the functionality that I need it is not in resop library, I can also implement my personal operation in resop, in a single JavaScript file.","title":"Developer"},{"location":"#hpc-user-researcher","text":"I am an HPC user that needs to run a batch of simulations with different initial conditions. I can use resop to submit multiple jobs from my machine using postman or other tools that facilitate the interaction with an API, without having to connect to the cluster directly. (in future versions) I use the loggin functionality of the API to keep track of my past simulations and organize them.","title":"HPC user / researcher"},{"location":"reference/","text":"Reference # The API reference to all the available endpoints implemented is built and tested with Postman . Postman # The live reference for the API can be found on: https://documenter.getpostman.com/view/19947146/VUjHNoik To import the library to Postman for usage, test and development, you can simply open the URL above and click on \"Run in Postman\". It'll ask you to download Postman but once that is done, you'll see the following: Click and import and on the next screen you'll find the postman collection:","title":"Reference"},{"location":"reference/#reference","text":"The API reference to all the available endpoints implemented is built and tested with Postman .","title":"Reference"},{"location":"reference/#postman","text":"The live reference for the API can be found on: https://documenter.getpostman.com/view/19947146/VUjHNoik To import the library to Postman for usage, test and development, you can simply open the URL above and click on \"Run in Postman\". It'll ask you to download Postman but once that is done, you'll see the following: Click and import and on the next screen you'll find the postman collection:","title":"Postman"},{"location":"develop/development/","text":"Development guide # Pre-requisites # NodeJS >= v12.20 MariaDB >= v15 Configure # See the deployment page for example configurations. Configuration to use the test cluster (vagrantcluster) # resop comes with a test cluster, outlined in the previous section , called vagrantcluster. The cluster can be used to test newly implemented functionalities in a virtual HPC environment, maximizing portability to a real environment. Once vagrantcluster is up and running, your .env file should look similar to: ### API settings ### # disable https for local testing ENABLE_HTTPS= 'false' # needed JWT_SECRET = 'random_secretString!' # diasable logging ENABLE_DB = 'false' # change this with your own cluster settings CLUSTER_NAME=\"devcluster\" CLUSTER_ADDRESS=\"127.0.0.1\" CLUSTER_SSH_PORT=\"2200\" # check this is the right port to access **fe1** VM # location where users private keys are saved, in the server where the API is hosted # see authentication in docs to know more CLUSTER_USERS_SSH_KEY_LOCATION=\"/tmp\" ### Operator settings ### # custom variables can be defined and used in user-implemented operators. ANYTHING=\"thatneedstobesecret\" Start the server # Resop can run in production or development mode. Local # Run the local server for development purposes with: npm run dev:watch This way, resop can be run \"clusterless\", meaning that the API will not attempt to connect to a cluster. Some features are disabled when running locally: Authentication Remote shell commands (both bare command endpoints and commands within an Operation) are not effectively submitted. A success reply will always be returned with the command string generated. dev:watch mode uses nodemon will monitor any changes and rebuild the project anytime a new modification is saved. Production # npm run prod Commands and operations are effetively submitted on the remote cluster after a connection attempt has made with the information provided in the JWT token in the HTTP request. The token is granted after sucessful authentication of a cluster user. Development environment # ESLint and Prettier are used for code formatting and help during development. See the configuration files in the repository base for more info. .vscode settings in the repository folder allow for automatic formatting when saving with the VS Code IDE.","title":"Development Guide"},{"location":"develop/development/#development-guide","text":"","title":"Development guide"},{"location":"develop/development/#pre-requisites","text":"NodeJS >= v12.20 MariaDB >= v15","title":"Pre-requisites"},{"location":"develop/development/#configure","text":"See the deployment page for example configurations.","title":"Configure"},{"location":"develop/development/#configuration-to-use-the-test-cluster-vagrantcluster","text":"resop comes with a test cluster, outlined in the previous section , called vagrantcluster. The cluster can be used to test newly implemented functionalities in a virtual HPC environment, maximizing portability to a real environment. Once vagrantcluster is up and running, your .env file should look similar to: ### API settings ### # disable https for local testing ENABLE_HTTPS= 'false' # needed JWT_SECRET = 'random_secretString!' # diasable logging ENABLE_DB = 'false' # change this with your own cluster settings CLUSTER_NAME=\"devcluster\" CLUSTER_ADDRESS=\"127.0.0.1\" CLUSTER_SSH_PORT=\"2200\" # check this is the right port to access **fe1** VM # location where users private keys are saved, in the server where the API is hosted # see authentication in docs to know more CLUSTER_USERS_SSH_KEY_LOCATION=\"/tmp\" ### Operator settings ### # custom variables can be defined and used in user-implemented operators. ANYTHING=\"thatneedstobesecret\"","title":"Configuration to use the test cluster (vagrantcluster)"},{"location":"develop/development/#start-the-server","text":"Resop can run in production or development mode.","title":"Start the server"},{"location":"develop/development/#local","text":"Run the local server for development purposes with: npm run dev:watch This way, resop can be run \"clusterless\", meaning that the API will not attempt to connect to a cluster. Some features are disabled when running locally: Authentication Remote shell commands (both bare command endpoints and commands within an Operation) are not effectively submitted. A success reply will always be returned with the command string generated. dev:watch mode uses nodemon will monitor any changes and rebuild the project anytime a new modification is saved.","title":"Local"},{"location":"develop/development/#production","text":"npm run prod Commands and operations are effetively submitted on the remote cluster after a connection attempt has made with the information provided in the JWT token in the HTTP request. The token is granted after sucessful authentication of a cluster user.","title":"Production"},{"location":"develop/development/#development-environment","text":"ESLint and Prettier are used for code formatting and help during development. See the configuration files in the repository base for more info. .vscode settings in the repository folder allow for automatic formatting when saving with the VS Code IDE.","title":"Development environment"},{"location":"develop/implementation/","text":"API implementation # Overview # respository : https://github.com/daviderovell0/resop/ resop is written in pure JavaScript and depends on some npm modules. Dependencies are listed in package.json . Components # We can split resop in 3 categories: API logic, backend and operators. API logic # # files involved: api.js env.js auth.js routes/* resop 's API logic is built with Express . The main idea is that endpoints representing commands and operations are not implemented in the routes, instead they define \"general\" endpoints that are imported dynamically from the operators, via the backend functions. Backend # # files involved : backend/* The classes Command and Operation provide methods to define respective actions. They make calls to the SSH client ( SSH2Utils ) class to perform remote operations on behalf of the authenticated user. Respective *Utils classes are used to import commands and operations defined in the operators folder. The SSH utility functions are implemented in C++ using the libssh2 library in the backend/addons* folder. They provide the core of the API for remote execution, they are exposed to NodeJS via the node-addon-api and then to Command and Operation logic via SSH2utils . All functions provided by this class are synchronous, the main reason behind implementing a custom SSH agent rather then using an existing one such as NPM's ssh2. Operators # # files involved: operators/* This is where the actual actions are defined: only this section needs to be edited when creating new API commands or operations, the API will import them automatically if their definition is correct and if the folder structure is respected. See here for more information on how to add new actions. Authentication # resop is stateless: everytime a user attempts authentication, resop attempts to connect via SSH to the remote cluster with the credentials given by the user. If the login is successful, it will generate a private/public keypair and map it to a JSON Web Token, which is return to the user. This allows the user, token owner to use the API and therefore access the remote cluster with the authenticated user credentials. Diagrams below outline the authentication process in detail. Successful authentication workflow: (1) Generate a public + (encrypted) private keypair - (2) copy the public key to the remote cluster - (3) store the private key locally, in the path set in the configuration file. The key name is generated by encrypting the username. Successive successful login attempts will therefore replace the existing key, making sure only one private key is stored at a time. - (4) Encode the generate private key passphrase, private key name and username into the JSON Web Token (JWT) - (5) Return the JWT to the user When the API receives a call, it reads the token embedded in the request. If valid, it extracts the informations that it needs to locate the key, unlock it and use it to SSH to the remote cluster with the username. It then runs the command/script and returns the response","title":"Implementation"},{"location":"develop/implementation/#api-implementation","text":"","title":"API implementation"},{"location":"develop/implementation/#overview","text":"respository : https://github.com/daviderovell0/resop/ resop is written in pure JavaScript and depends on some npm modules. Dependencies are listed in package.json .","title":"Overview"},{"location":"develop/implementation/#components","text":"We can split resop in 3 categories: API logic, backend and operators.","title":"Components"},{"location":"develop/implementation/#api-logic","text":"# files involved: api.js env.js auth.js routes/* resop 's API logic is built with Express . The main idea is that endpoints representing commands and operations are not implemented in the routes, instead they define \"general\" endpoints that are imported dynamically from the operators, via the backend functions.","title":"API logic"},{"location":"develop/implementation/#backend","text":"# files involved : backend/* The classes Command and Operation provide methods to define respective actions. They make calls to the SSH client ( SSH2Utils ) class to perform remote operations on behalf of the authenticated user. Respective *Utils classes are used to import commands and operations defined in the operators folder. The SSH utility functions are implemented in C++ using the libssh2 library in the backend/addons* folder. They provide the core of the API for remote execution, they are exposed to NodeJS via the node-addon-api and then to Command and Operation logic via SSH2utils . All functions provided by this class are synchronous, the main reason behind implementing a custom SSH agent rather then using an existing one such as NPM's ssh2.","title":"Backend"},{"location":"develop/implementation/#operators","text":"# files involved: operators/* This is where the actual actions are defined: only this section needs to be edited when creating new API commands or operations, the API will import them automatically if their definition is correct and if the folder structure is respected. See here for more information on how to add new actions.","title":"Operators"},{"location":"develop/implementation/#authentication","text":"resop is stateless: everytime a user attempts authentication, resop attempts to connect via SSH to the remote cluster with the credentials given by the user. If the login is successful, it will generate a private/public keypair and map it to a JSON Web Token, which is return to the user. This allows the user, token owner to use the API and therefore access the remote cluster with the authenticated user credentials. Diagrams below outline the authentication process in detail. Successful authentication workflow: (1) Generate a public + (encrypted) private keypair - (2) copy the public key to the remote cluster - (3) store the private key locally, in the path set in the configuration file. The key name is generated by encrypting the username. Successive successful login attempts will therefore replace the existing key, making sure only one private key is stored at a time. - (4) Encode the generate private key passphrase, private key name and username into the JSON Web Token (JWT) - (5) Return the JWT to the user When the API receives a call, it reads the token embedded in the request. If valid, it extracts the informations that it needs to locate the key, unlock it and use it to SSH to the remote cluster with the username. It then runs the command/script and returns the response","title":"Authentication"},{"location":"develop/vagrantcluster/","text":"Vagrant cluster # resop comes with \"recipes\" to build an HPC test cluster with 6 VMs that can be hosted on a local machine for development purposes. Pre-requisites # Vagrant Ansible VirtualBox Install & use # Check the README file in the vagrantcluster/ folder in the main repository.","title":"Local test cluster"},{"location":"develop/vagrantcluster/#vagrant-cluster","text":"resop comes with \"recipes\" to build an HPC test cluster with 6 VMs that can be hosted on a local machine for development purposes.","title":"Vagrant cluster"},{"location":"develop/vagrantcluster/#pre-requisites","text":"Vagrant Ansible VirtualBox","title":"Pre-requisites"},{"location":"develop/vagrantcluster/#install-use","text":"Check the README file in the vagrantcluster/ folder in the main repository.","title":"Install &amp; use"},{"location":"install/deploy/","text":"Deployment # The API can be hosted anywhere with SSH access to the cluster. Obviously, this includes any local machine, for personal use only. Configuration # Configuration is made via environment variables taken from the .env file in the main repository directory. The API will automatically load the environment variable when starting. Rename .env.example to .env and fill variables with the corresponding values for you setup. An example is reported below: ### API settings ### # The API can be set to use HTTPS by setting this # to 'true'. if the variable is not set or set to # any other value, the API will use HTTP instead ENABLE_HTTPS= 'true' # if ENABLE_HTTPS= 'true' set # the variables below to point at the SSL files SSL_PRIVATE_KEY = '/path/to/your/key' SSL_CERTIFICATE = 'path/to/your/certificate' # string used to encrypt JSON Web tokens (JWT) for authentication # This value should not be disclosed. JWT_SECRET = 'random_secretString!' # Enable the database for logging (@TODO) or custom objects to be stored (@TODO) # if ENABLE_DB = 'true', an existing MariaDB database needs to be accessible by the API ENABLE_DB = 'true' # MariaDB db credentials for database functionalities # vars below ignored if ENABLE_DB != 'true' DB = 'myDB' DB_USER = 'myDBUser' DB_PWD = 'myDBPassword' DB_HOST = 'localhost' DB_PORT = '3306' DB_LOGGING = false # Cluster access # change this with your own cluster settings CLUSTER_NAME=\"vagrantcluster\" CLUSTER_ADDRESS=\"127.0.0.1\" CLUSTER_SSH_PORT=\"2200\" # location where users private keys are saved, in the server where the API is hosted # see authentication in docs to know more CLUSTER_USERS_SSH_KEY_LOCATION=\"/tmp\" ### Operator settings ### # custom variables can be defined and used in user-implemented operators. ANYTHING=\"thatneedstobesecret\" All variables under API setings need to be set, unless ENABLE_HTTPS or ENABLE_DB' are set to false . In that case, SSL and DB variables respectively can be left empty. resop will run checks on the configuration file before starting the application, check the logs for additional information. Start the API # On production environnements run: npm run prod If the configuration is valid, resop will attempt to start at port 3000 of the current server: https://myhost.com:3000 . If the API is running correctly, you should see the following message when accessing the server instance: message \"Welcome to resop API. I am ready to work!","title":"Deployment"},{"location":"install/deploy/#deployment","text":"The API can be hosted anywhere with SSH access to the cluster. Obviously, this includes any local machine, for personal use only.","title":"Deployment"},{"location":"install/deploy/#configuration","text":"Configuration is made via environment variables taken from the .env file in the main repository directory. The API will automatically load the environment variable when starting. Rename .env.example to .env and fill variables with the corresponding values for you setup. An example is reported below: ### API settings ### # The API can be set to use HTTPS by setting this # to 'true'. if the variable is not set or set to # any other value, the API will use HTTP instead ENABLE_HTTPS= 'true' # if ENABLE_HTTPS= 'true' set # the variables below to point at the SSL files SSL_PRIVATE_KEY = '/path/to/your/key' SSL_CERTIFICATE = 'path/to/your/certificate' # string used to encrypt JSON Web tokens (JWT) for authentication # This value should not be disclosed. JWT_SECRET = 'random_secretString!' # Enable the database for logging (@TODO) or custom objects to be stored (@TODO) # if ENABLE_DB = 'true', an existing MariaDB database needs to be accessible by the API ENABLE_DB = 'true' # MariaDB db credentials for database functionalities # vars below ignored if ENABLE_DB != 'true' DB = 'myDB' DB_USER = 'myDBUser' DB_PWD = 'myDBPassword' DB_HOST = 'localhost' DB_PORT = '3306' DB_LOGGING = false # Cluster access # change this with your own cluster settings CLUSTER_NAME=\"vagrantcluster\" CLUSTER_ADDRESS=\"127.0.0.1\" CLUSTER_SSH_PORT=\"2200\" # location where users private keys are saved, in the server where the API is hosted # see authentication in docs to know more CLUSTER_USERS_SSH_KEY_LOCATION=\"/tmp\" ### Operator settings ### # custom variables can be defined and used in user-implemented operators. ANYTHING=\"thatneedstobesecret\" All variables under API setings need to be set, unless ENABLE_HTTPS or ENABLE_DB' are set to false . In that case, SSL and DB variables respectively can be left empty. resop will run checks on the configuration file before starting the application, check the logs for additional information.","title":"Configuration"},{"location":"install/deploy/#start-the-api","text":"On production environnements run: npm run prod If the configuration is valid, resop will attempt to start at port 3000 of the current server: https://myhost.com:3000 . If the API is running correctly, you should see the following message when accessing the server instance: message \"Welcome to resop API. I am ready to work!","title":"Start the API"},{"location":"install/install/","text":"Installation # resop can be used for any Linux cluster, but it is tailored for High Performance Computing (HPC) clusters. Pre-requisites # Host # Windows and MacOS not tested, likely compatible but no installation instructions are provided Linux NodeJS >= v12.20 CMake gcc >= 9.20 (set as default compiler recommended) OpenSSL For future logging functionality only (not in current version): MariaDB >= v15 Remote cluster # Linux-based OS Default shell: bash SSH enabled and accessible by resop API host. Installation # Install dependecies # Clone the repository git clone https://github.com/daviderovell0/resop.git Install libssh2 git clone https://github.com/libssh2/libssh2.git cd libssh2 mkdir build && cd build cmake .. make make install Install NPM dependencies cd resop npm install Create a database and a user # For future logging functionality only (not in current version) Access the MariaDB shell sudo mariadb Create an empty MariaSQL database for the app CREATE USER admin@localhost IDENTIFIED BY 'admin'; CREATE DATABASE api_db; GRANT ALL PRIVILEGES on api_db.* TO 'admin'@'localhost';","title":"Installation"},{"location":"install/install/#installation","text":"resop can be used for any Linux cluster, but it is tailored for High Performance Computing (HPC) clusters.","title":"Installation"},{"location":"install/install/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"install/install/#host","text":"Windows and MacOS not tested, likely compatible but no installation instructions are provided Linux NodeJS >= v12.20 CMake gcc >= 9.20 (set as default compiler recommended) OpenSSL For future logging functionality only (not in current version): MariaDB >= v15","title":"Host"},{"location":"install/install/#remote-cluster","text":"Linux-based OS Default shell: bash SSH enabled and accessible by resop API host.","title":"Remote cluster"},{"location":"install/install/#installation_1","text":"","title":"Installation"},{"location":"install/install/#install-dependecies","text":"Clone the repository git clone https://github.com/daviderovell0/resop.git Install libssh2 git clone https://github.com/libssh2/libssh2.git cd libssh2 mkdir build && cd build cmake .. make make install Install NPM dependencies cd resop npm install","title":"Install dependecies"},{"location":"install/install/#create-a-database-and-a-user","text":"For future logging functionality only (not in current version) Access the MariaDB shell sudo mariadb Create an empty MariaSQL database for the app CREATE USER admin@localhost IDENTIFIED BY 'admin'; CREATE DATABASE api_db; GRANT ALL PRIVILEGES on api_db.* TO 'admin'@'localhost';","title":"Create a database and a user"},{"location":"use/auth/","text":"resop is a stateless API: user management and authentication are not managed via a database. Instead, it leverages on the authentication of the remote Linux cluster. It can be seen as a SSH client: a user can access the API (and therefore access the remote cluster) with the SSH credentials used to connect to that cluster. Consequently, all the commands and operations run via the API will have the same priviledges and rights of the authenticated user running that operation. Check the authentication section under Develop for more informaton on the internal authentication process and security considerations. Process # resop uses JSON Web Tokens (JWTs) for authentication: Login requests by submitting a POST request to {API_address}/auth/opn/login , providing username and password in the request body. Example: curl -X POST -d \"username=value1&password=value2\" https://resop:3000/auth/opn/login If authentication is successfull, the API will issue a JWT token: # API response { \"success\": true, \"operation\": \"auth:login\", \"output\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImRyb3ZlbGxpIiwia2V5UGF03336Ii9ob21lL2RhdmlkZXJvdmVsbDAvcmVzb3BrZXlzL2Y4OWJhMDg1M2M5Njc5YzAiLCJrZXlQYXNzcGhyYXNlIjoiS0YwbHRUb0dPSjZWT0YiLCJpYXQiOjE2NDY3NTczODl9._9kKcb3iM_6_2bKS_VXPcu-NlxNWoin5fFc4C2Omysg\", \"log\": [ \"user drovelli attempting to login...\", \"successful login\" ] } The JWT token can be embedded in the HTTP header following requests to protected endpoints*, as a Bearer Token. Check https://jwt.io/introduction for more information on token-based authentication with JWTs. Protected endpoints* # All endpoints apart from / (home) /auth/opn/login (login)","title":"Authentication"},{"location":"use/auth/#process","text":"resop uses JSON Web Tokens (JWTs) for authentication: Login requests by submitting a POST request to {API_address}/auth/opn/login , providing username and password in the request body. Example: curl -X POST -d \"username=value1&password=value2\" https://resop:3000/auth/opn/login If authentication is successfull, the API will issue a JWT token: # API response { \"success\": true, \"operation\": \"auth:login\", \"output\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImRyb3ZlbGxpIiwia2V5UGF03336Ii9ob21lL2RhdmlkZXJvdmVsbDAvcmVzb3BrZXlzL2Y4OWJhMDg1M2M5Njc5YzAiLCJrZXlQYXNzcGhyYXNlIjoiS0YwbHRUb0dPSjZWT0YiLCJpYXQiOjE2NDY3NTczODl9._9kKcb3iM_6_2bKS_VXPcu-NlxNWoin5fFc4C2Omysg\", \"log\": [ \"user drovelli attempting to login...\", \"successful login\" ] } The JWT token can be embedded in the HTTP header following requests to protected endpoints*, as a Bearer Token. Check https://jwt.io/introduction for more information on token-based authentication with JWTs.","title":"Process"},{"location":"use/auth/#protected-endpoints","text":"All endpoints apart from / (home) /auth/opn/login (login)","title":"Protected endpoints*"},{"location":"use/customise/","text":"Implement custom commands or operations # If the command or operation that you are looking for is not already in resop, you can implement your own one. The API is structured in a way to ease the implementation of new endpoints as much as possible: Commands are implemented with a few lines of YAML Operations are implemented on single JavaScript file. This page is a guide to add a new endpoint. Help build a library of useful functional by adding your endpoints to the official repo! Operators # Commands and operations are grouped into \"operators\". Operators serve uniquely as logical categories for better organisation. In practice, operators are just folders located in src/operators/ . New commands or operations can be added to existing operators, or a new operator can be created. The content of the operator folder needs to follow a precise structure: operator/ \u251c\u2500\u2500 commands.yml \u2514\u2500\u2500 operations/ \u251c\u2500\u2500 operation1.js \u2514\u2500\u2500 operation2.js At runtime, the API will follow this structure to import and build new endpoints automatically. Multiple commands are defined in the commands.yml file, while each operation is defined using separate JS file. For example, a command named command1 inside commands.yml in the above structure will generate the following endpoint: https://<API>/operator/cmd/command1 while the 2 operations will generate: https://<API>/operator/opn/operation1 https://<API>/operator/opn/operation2 The template operator can be used to build new commands and operations! Command # Commands are defined in a YAML file called commands.yml , in the corresponding operator folder. Multiple commands can be defined in the same file. To add a new command, simply edit the commands.yml file and add a new entry, following the template below: - command: test # the command to run options: # the command options option1: flag: -o1 # the option flag description: option 1 description # an optional description option2: flag: -o2 description: option 2 description # form: static/slurm_run_form.html # extra feature not included description: This is a description # an optional description # enabled: true # not currently used, but can be used to disable certain commands temporarily The input field does not need to be specified: it is automatically added by the API for additional arguments or stdin for each command. Implicitly, the command must correspond to an existing command or script in the remote cluster. Non-declared-options can be used to filter out unwanted options and flags, allowing admins to put additional control on endpoints. Operations # Operations are defined in a JavaScript file in the corresponding operator directory. The file name must correspond to the name of the operation. I.e. myOperation.js -> /{operator}/opn/myOperation . To create or edit an operation, use an instance of the Operation class. This class provides methods to define your operation, the template example is reported below: import Operation from '../../../backend/Operation'; import * as otherOPN from '../../operator/operations/otherOPN'; /** * @summary Description */ // (mustdo) Instatiate a new Operation: const opn = new Operation(); opn.setDescription('Creates a user in the api database'); // options in the POST request body, to be used in the Operation logic opn.defineOptions({ option1: '-> description', option2: '-> description', }); // Create an execution function that will constitute the body of the operation. // This fucntion will be executed on a POST request to this operation corresponding // endpoint. // Can contain any arbitrary logic, be sync or async function exec() { // an Operation can run a command in the remote cluster and read its stdout. // The command is executed on behalf of the API user (=cluser user) running it. // Error handling is NOT need on commands, it is already done by the Operation. // Commands can be of 2 types: // standard strings const stdout1 = opn.runCommand('my_command --flag option input'); // use a command defined in this operator's commands, as it would be in a POST request const stdout2 = opn.runCommandDefined({ command: 'command_name', option1: 'option_argument', input: 'stdin for command name', }); // add log information to the HTTP response and API log opn.addLog('commands completed...'); // any JS code can be used, including exeternal modules let out = stdout2; out = `${out}, ${stdout1}`; // Error handling via .error() method. The Opeation will be interruped at the first .error // found if (out === null) { opn.error('no output from commands!'); } /** * other operation can be imported from the correspoding file (see example import at * the top of the file). Then it can be used as a command, error handling is also taken care * by resop */ const passwordHash = opn.runOperation(otherOPN, { field: opn.options.option1, option: 'true', }); opn.addLog(passwordHash); // the result of the operation return out; } // (must do), set the execution function opn.set(exec); // (mustdo) export the Operation instance export default opn; Follow the comments for an explaination of each step. A few notes # Embedded operations are supported. No error handling is needed in the operation execution function definition: resop does that automatically use addLog() to \"track progress\" or log important information. Log will be displayed as a field of the API response. use error() to exit the operation upon some chosen conditions. A formatted response will be automatically generated by the API for now, await s are compulsory on all Operation methods. The execution function must also be async 'ed. Once defined, the API will automatically create the corresponding endpoint. See the overview for information on the endpoint layout.","title":"Customise"},{"location":"use/customise/#implement-custom-commands-or-operations","text":"If the command or operation that you are looking for is not already in resop, you can implement your own one. The API is structured in a way to ease the implementation of new endpoints as much as possible: Commands are implemented with a few lines of YAML Operations are implemented on single JavaScript file. This page is a guide to add a new endpoint. Help build a library of useful functional by adding your endpoints to the official repo!","title":"Implement custom commands or operations"},{"location":"use/customise/#operators","text":"Commands and operations are grouped into \"operators\". Operators serve uniquely as logical categories for better organisation. In practice, operators are just folders located in src/operators/ . New commands or operations can be added to existing operators, or a new operator can be created. The content of the operator folder needs to follow a precise structure: operator/ \u251c\u2500\u2500 commands.yml \u2514\u2500\u2500 operations/ \u251c\u2500\u2500 operation1.js \u2514\u2500\u2500 operation2.js At runtime, the API will follow this structure to import and build new endpoints automatically. Multiple commands are defined in the commands.yml file, while each operation is defined using separate JS file. For example, a command named command1 inside commands.yml in the above structure will generate the following endpoint: https://<API>/operator/cmd/command1 while the 2 operations will generate: https://<API>/operator/opn/operation1 https://<API>/operator/opn/operation2 The template operator can be used to build new commands and operations!","title":"Operators"},{"location":"use/customise/#command","text":"Commands are defined in a YAML file called commands.yml , in the corresponding operator folder. Multiple commands can be defined in the same file. To add a new command, simply edit the commands.yml file and add a new entry, following the template below: - command: test # the command to run options: # the command options option1: flag: -o1 # the option flag description: option 1 description # an optional description option2: flag: -o2 description: option 2 description # form: static/slurm_run_form.html # extra feature not included description: This is a description # an optional description # enabled: true # not currently used, but can be used to disable certain commands temporarily The input field does not need to be specified: it is automatically added by the API for additional arguments or stdin for each command. Implicitly, the command must correspond to an existing command or script in the remote cluster. Non-declared-options can be used to filter out unwanted options and flags, allowing admins to put additional control on endpoints.","title":"Command"},{"location":"use/customise/#operations","text":"Operations are defined in a JavaScript file in the corresponding operator directory. The file name must correspond to the name of the operation. I.e. myOperation.js -> /{operator}/opn/myOperation . To create or edit an operation, use an instance of the Operation class. This class provides methods to define your operation, the template example is reported below: import Operation from '../../../backend/Operation'; import * as otherOPN from '../../operator/operations/otherOPN'; /** * @summary Description */ // (mustdo) Instatiate a new Operation: const opn = new Operation(); opn.setDescription('Creates a user in the api database'); // options in the POST request body, to be used in the Operation logic opn.defineOptions({ option1: '-> description', option2: '-> description', }); // Create an execution function that will constitute the body of the operation. // This fucntion will be executed on a POST request to this operation corresponding // endpoint. // Can contain any arbitrary logic, be sync or async function exec() { // an Operation can run a command in the remote cluster and read its stdout. // The command is executed on behalf of the API user (=cluser user) running it. // Error handling is NOT need on commands, it is already done by the Operation. // Commands can be of 2 types: // standard strings const stdout1 = opn.runCommand('my_command --flag option input'); // use a command defined in this operator's commands, as it would be in a POST request const stdout2 = opn.runCommandDefined({ command: 'command_name', option1: 'option_argument', input: 'stdin for command name', }); // add log information to the HTTP response and API log opn.addLog('commands completed...'); // any JS code can be used, including exeternal modules let out = stdout2; out = `${out}, ${stdout1}`; // Error handling via .error() method. The Opeation will be interruped at the first .error // found if (out === null) { opn.error('no output from commands!'); } /** * other operation can be imported from the correspoding file (see example import at * the top of the file). Then it can be used as a command, error handling is also taken care * by resop */ const passwordHash = opn.runOperation(otherOPN, { field: opn.options.option1, option: 'true', }); opn.addLog(passwordHash); // the result of the operation return out; } // (must do), set the execution function opn.set(exec); // (mustdo) export the Operation instance export default opn; Follow the comments for an explaination of each step.","title":"Operations"},{"location":"use/customise/#a-few-notes","text":"Embedded operations are supported. No error handling is needed in the operation execution function definition: resop does that automatically use addLog() to \"track progress\" or log important information. Log will be displayed as a field of the API response. use error() to exit the operation upon some chosen conditions. A formatted response will be automatically generated by the API for now, await s are compulsory on all Operation methods. The execution function must also be async 'ed. Once defined, the API will automatically create the corresponding endpoint. See the overview for information on the endpoint layout.","title":"A few notes"},{"location":"use/overview/","text":"Overview # The purpose of this API is to perform (customisable) \"actions\" onto a remote HPC cluster. There are 2 types of possible actions: commands and operations : Command : a single, atomic shell command such as cp a.txt b.txt Operation : a more complex action that could consist in several commands followed by some logic, such as taking the ouput of the first command, processing it and feeding into stdin of the second command. Both actions can belong to a specific operator : a group of commands and operations that have common utility. All endpoints implementing commands and operations follow this structure: All API communcation is done via JSON format. All operators are defined in src/operators in the source code. Check the reference for more information Commands # Three types of HTTP requests are available for a command: GET {API}/{operator}/commands # Lists all the supported commands for the given {operator} GET {API}/{operator}/cmd/:{command} # Fetches a single {command} info and supported options for the given {operator} POST {API}/{operator}/cmd/:{command} # Runs the actual command. Allowed options (flags) can be defined in the POST body. Note that not all flags of a specific command might be available: a command definition in the API might choose to filter out some options. { \"input\": \"input\", \"option1\": \"value\", \"option2\": \"value\", ... } Note : input = stdin . Arguments of commands are always taken from the input field for the submitted POST request body. Operations # Three types of HTTP requests are available for an operation : GET {API}/{operator}/operations # Lists all the supported operaions for the given {operator} GET {API}/{operator}/opn/:{operation} # Fetches a single {operation} info and supported options for the given {operator} POST {API}/{operator}/opn/:{operation} # Runs the actual operation. Options can be defined in the POST body in the same way as commands: { \"option1\": \"value\", \"option2\": \"value\", \"option3\": \"value\", ... } Supported operations with options are defined via JavaScript files named after the operation . All operations for a given operator are defined in operators/{operator}/operations/{myOperation}.js in the source code.","title":"Overview"},{"location":"use/overview/#overview","text":"The purpose of this API is to perform (customisable) \"actions\" onto a remote HPC cluster. There are 2 types of possible actions: commands and operations : Command : a single, atomic shell command such as cp a.txt b.txt Operation : a more complex action that could consist in several commands followed by some logic, such as taking the ouput of the first command, processing it and feeding into stdin of the second command. Both actions can belong to a specific operator : a group of commands and operations that have common utility. All endpoints implementing commands and operations follow this structure: All API communcation is done via JSON format. All operators are defined in src/operators in the source code. Check the reference for more information","title":"Overview"},{"location":"use/overview/#commands","text":"Three types of HTTP requests are available for a command:","title":"Commands"},{"location":"use/overview/#get-apioperatorcommands","text":"Lists all the supported commands for the given {operator}","title":"GET {API}/{operator}/commands"},{"location":"use/overview/#get-apioperatorcmdcommand","text":"Fetches a single {command} info and supported options for the given {operator}","title":"GET {API}/{operator}/cmd/:{command}"},{"location":"use/overview/#post-apioperatorcmdcommand","text":"Runs the actual command. Allowed options (flags) can be defined in the POST body. Note that not all flags of a specific command might be available: a command definition in the API might choose to filter out some options. { \"input\": \"input\", \"option1\": \"value\", \"option2\": \"value\", ... } Note : input = stdin . Arguments of commands are always taken from the input field for the submitted POST request body.","title":"POST {API}/{operator}/cmd/:{command}"},{"location":"use/overview/#operations","text":"Three types of HTTP requests are available for an operation :","title":"Operations"},{"location":"use/overview/#get-apioperatoroperations","text":"Lists all the supported operaions for the given {operator}","title":"GET {API}/{operator}/operations"},{"location":"use/overview/#get-apioperatoropnoperation","text":"Fetches a single {operation} info and supported options for the given {operator}","title":"GET {API}/{operator}/opn/:{operation}"},{"location":"use/overview/#post-apioperatoropnoperation","text":"Runs the actual operation. Options can be defined in the POST body in the same way as commands: { \"option1\": \"value\", \"option2\": \"value\", \"option3\": \"value\", ... } Supported operations with options are defined via JavaScript files named after the operation . All operations for a given operator are defined in operators/{operator}/operations/{myOperation}.js in the source code.","title":"POST {API}/{operator}/opn/:{operation}"},{"location":"use/usage/","text":"Usage guidance # resop uses a modular structure to organise endpoints. Each endpoints correspond either to a command or an operation. Commands and operations are both subpaths of an operator: /{operator}/cmd/{command} <- command /{operator}/opn/{operation} <- operation More details about endpoints in the overview . Most endpoints are protected and can be accessed only with a token post-authentication. More in the auth section. A list of all the available endpoints can be found in the reference . API calls (requests) # Each endpoint can be called with both GET and POST. GET requests return informative data about the endpoint: description and available options. They are internal to the API and will not interact with the remote cluster. POST requests perform an action and return the its result output with additional information such as possible error details and logs. They often, but not always, interact with the remote cluster. Key-value options in the request body (i.e. form fields) for both commands and operations allow for specific options. Some option might be mandatory depending on the implementation of the action. All fields are sanitized to filter special shell characters ; this is done to avoid command injection. A good workflow is to use GET to retrieve the available options and then use POST with the right options. Additional GET (only) endpoints in the following format: /{operator}/commands /{operator}/operations return a list of available commands or operations, respectively. Response object # In the current version, output and error responses might not be consistent. Below are possible examples of responses Command # GET # { \"command\": \"salloc\", \"description\": \"salloc is used to allocate a Slurm job allocation\", \"options\": { \"account\": { \"description\": \"Charge resources used by this job to specified account\", \"flag\": \"--account\" }, ... \"exclusive\": { \"description\": \"The job allocation can not share nodes with other running jobs\", \"flag\": \"--exclusive\" }, ... } } POST # Execution error { \"success\": false, \"command\": \"ldapsearch -b \\\"dc=xf,dc=priv\\\" -LLL -x \\\"cn=testproject\\\"\", \"output\": \"bash: ldapsearch: command not found\\n\" } InputFormatError { \"success\": false, \"input\": \"\\\"cn=testproject\\\" &\", \"output\": \"InputFormatError: character & not allowed, escape it with a backslash\" } Successful command { \"success\": true, \"command\": \"salloc \", \"output\": 0 } Operation # GET # { \"name\": \"sbatchd\", \"operator\": \"slurm\", \"description\": \"sbatch direct: sbatch taking an input files as option\", \"options\": { \"script\": \"SBATCH submission script (as string)\" } } POST # Successful request { \"success\": true, \"operation\": \"linux-user:hashMD5\", \"output\": \"\", \"log\": [ \"Hash generation: MD5...\", \"adding escape characters...\" ] } OperationError: an error coming from the operation execution { \"success\": false, \"operation\": \"linux-user:hashMD5\", \"output\": \"OperationError\", \"log\": [ \"invalid escapeChar type\" ] } InternalError: something in the code is not right. It could be the operation definition { \"success\": false, \"operation\": \"linux-user:createUser\", \"output\": \"InternalError\", \"log\": \"message\" } core/ endpoints # Special \"general\" endpoints are leaves of the core/ operator. These endpoints provide direct access to the main SSH functionalities: core/opn/run : run any remote command without shell special characters filter (pipe, & etc. allowed) core/opn/getFile , core/opn/sendFile : scp files from and to the remote cluster core/opn/getFolder , core/opn/sendFolder : scp doler from and to the remote cluster Other endpoints are specific and can be used (or built) to perform complex shell operations.","title":"Usage"},{"location":"use/usage/#usage-guidance","text":"resop uses a modular structure to organise endpoints. Each endpoints correspond either to a command or an operation. Commands and operations are both subpaths of an operator: /{operator}/cmd/{command} <- command /{operator}/opn/{operation} <- operation More details about endpoints in the overview . Most endpoints are protected and can be accessed only with a token post-authentication. More in the auth section. A list of all the available endpoints can be found in the reference .","title":"Usage guidance"},{"location":"use/usage/#api-calls-requests","text":"Each endpoint can be called with both GET and POST. GET requests return informative data about the endpoint: description and available options. They are internal to the API and will not interact with the remote cluster. POST requests perform an action and return the its result output with additional information such as possible error details and logs. They often, but not always, interact with the remote cluster. Key-value options in the request body (i.e. form fields) for both commands and operations allow for specific options. Some option might be mandatory depending on the implementation of the action. All fields are sanitized to filter special shell characters ; this is done to avoid command injection. A good workflow is to use GET to retrieve the available options and then use POST with the right options. Additional GET (only) endpoints in the following format: /{operator}/commands /{operator}/operations return a list of available commands or operations, respectively.","title":"API calls (requests)"},{"location":"use/usage/#response-object","text":"In the current version, output and error responses might not be consistent. Below are possible examples of responses","title":"Response object"},{"location":"use/usage/#command","text":"","title":"Command"},{"location":"use/usage/#get","text":"{ \"command\": \"salloc\", \"description\": \"salloc is used to allocate a Slurm job allocation\", \"options\": { \"account\": { \"description\": \"Charge resources used by this job to specified account\", \"flag\": \"--account\" }, ... \"exclusive\": { \"description\": \"The job allocation can not share nodes with other running jobs\", \"flag\": \"--exclusive\" }, ... } }","title":"GET"},{"location":"use/usage/#post","text":"Execution error { \"success\": false, \"command\": \"ldapsearch -b \\\"dc=xf,dc=priv\\\" -LLL -x \\\"cn=testproject\\\"\", \"output\": \"bash: ldapsearch: command not found\\n\" } InputFormatError { \"success\": false, \"input\": \"\\\"cn=testproject\\\" &\", \"output\": \"InputFormatError: character & not allowed, escape it with a backslash\" } Successful command { \"success\": true, \"command\": \"salloc \", \"output\": 0 }","title":"POST"},{"location":"use/usage/#operation","text":"","title":"Operation"},{"location":"use/usage/#get_1","text":"{ \"name\": \"sbatchd\", \"operator\": \"slurm\", \"description\": \"sbatch direct: sbatch taking an input files as option\", \"options\": { \"script\": \"SBATCH submission script (as string)\" } }","title":"GET"},{"location":"use/usage/#post_1","text":"Successful request { \"success\": true, \"operation\": \"linux-user:hashMD5\", \"output\": \"\", \"log\": [ \"Hash generation: MD5...\", \"adding escape characters...\" ] } OperationError: an error coming from the operation execution { \"success\": false, \"operation\": \"linux-user:hashMD5\", \"output\": \"OperationError\", \"log\": [ \"invalid escapeChar type\" ] } InternalError: something in the code is not right. It could be the operation definition { \"success\": false, \"operation\": \"linux-user:createUser\", \"output\": \"InternalError\", \"log\": \"message\" }","title":"POST"},{"location":"use/usage/#core-endpoints","text":"Special \"general\" endpoints are leaves of the core/ operator. These endpoints provide direct access to the main SSH functionalities: core/opn/run : run any remote command without shell special characters filter (pipe, & etc. allowed) core/opn/getFile , core/opn/sendFile : scp files from and to the remote cluster core/opn/getFolder , core/opn/sendFolder : scp doler from and to the remote cluster Other endpoints are specific and can be used (or built) to perform complex shell operations.","title":"core/ endpoints"}]}